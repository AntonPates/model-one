{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4613b32e",
   "metadata": {},
   "source": [
    "### Data\n",
    "- S&P 500 companies list https://www.kaggle.com/datasets/andrewmvd/sp-500-stocks\n",
    "- S&P 500 stock prices yfinance library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca824ba",
   "metadata": {},
   "source": [
    "### Stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de57b90ec15b62",
   "metadata": {},
   "source": [
    "#### Get stock prices history for the whole period till now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6525fecfc57085a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T16:20:26.071868Z",
     "start_time": "2024-10-06T16:10:45.720027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done voo\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "tickers_log = pd.read_csv('news/constituents.csv')\n",
    "\n",
    "tickers_log = tickers_log['Symbol'].str.lower().sort_values().tolist()\n",
    "#  create dir for current_date\n",
    "current_date = datetime.datetime.now().date().strftime(\"%Y%m%d\")\n",
    "os.makedirs(f'data/{current_date}', exist_ok=True)\n",
    "for t in tickers_log:\n",
    "    ticker = yf.Ticker(t)\n",
    "    h = ticker.history(period=\"max\")\n",
    "    h.to_csv(f'data/{current_date}/' + t + \".csv\", sep=\",\")\n",
    "    print(\"Done\", t)\n",
    "    time.sleep(.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50712dfd",
   "metadata": {},
   "source": [
    "#### Fill table stocks in stocks database with data from yfinance library output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3343686",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T16:37:39.035666Z",
     "start_time": "2024-10-06T16:32:26.297503Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3, os\n",
    "conn = sqlite3.connect('data/stocks.db')\n",
    "c = conn.cursor()\n",
    "c.execute(\"DROP TABLE IF EXISTS stocks\")\n",
    "c.execute(\"DROP INDEX IF EXISTS idx_unique_stocks_ticker_date\")\n",
    "c.execute(\"CREATE TABLE EXISTS stocks (date text, ticker text, open real, high real, low real, close real, volume real, dividends real, stocks_split real) STRICT\")\n",
    "c.execute('CREATE UNIQUE INDEX idx_unique_stocks_ticker_date ON stocks(ticker,date)')\n",
    "# write to database data from csv\n",
    "df = pd.read_csv('./news/constituents.csv')\n",
    "for index, row in df.iterrows():\n",
    "    ticker = row['Symbol'].lower()\n",
    "    fname = './data/20241209/{}.csv'.format(ticker)\n",
    "    if not os.path.exists(fname):\n",
    "        print('File {} not found'.format(fname))\n",
    "        continue\n",
    "    df = pd.read_csv(fname)\n",
    "    for index, row in df.iterrows():\n",
    "        date = row['Date'][:10]\n",
    "        open = row['Open']\n",
    "        high = row['High']\n",
    "        low = row['Low']\n",
    "        close = row['Close']\n",
    "        volume = row['Volume']\n",
    "        dividends = row['Dividends']\n",
    "        stocks_split = row['Stock Splits']\n",
    "        c.execute(\"INSERT INTO stocks (date, ticker, open, high, low, close, volume, dividends, stocks_split) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\", \n",
    "                  (date, ticker, open, high, low, close, volume, dividends, stocks_split))\n",
    "c.close()\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d082a68c",
   "metadata": {},
   "source": [
    "#### Get latest data from yfinance library and update table stocks in stocks database\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc87f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import sqlite3, os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "conn = sqlite3.connect('data/stocks.db')\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT DISTINCT(ticker) FROM stocks ORDER BY ticker')\n",
    "tickers_log = c.fetchall()\n",
    "\n",
    "for ticker in tickers_log:\n",
    "    ticker = ticker[0]\n",
    "    print('Processing ticker {}'.format(ticker))\n",
    "    c.execute('SELECT max(date) FROM stocks WHERE ticker = ?', (ticker,))\n",
    "    max_date = c.fetchone()[0]\n",
    "    if max_date is None:\n",
    "        print('Need to get the whole history for this ticker {}'.format(ticker))\n",
    "        yahoo_ticker = yf.Ticker(ticker)\n",
    "        data = yahoo_ticker.history(period=\"max\")\n",
    "        for idx, row in data.iterrows():\n",
    "            date = idx.date()\n",
    "            open = row['Open']\n",
    "            high = row['High']\n",
    "            low = row['Low']\n",
    "            close = row['Close']\n",
    "            volume = row['Volume']\n",
    "            dividends = row['Dividends']\n",
    "            stocks_split = row['Stock Splits']\n",
    "            c.execute(\"INSERT INTO stocks (date, ticker, open, high, low, close, volume, dividends, stocks_split) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\", \n",
    "                    (date.strftime(\"%Y-%m-%d\"), ticker, open, high, low, close, volume, dividends, stocks_split))\n",
    "        data.to_csv(f'data/yahoo_finance/' + ticker + \".csv\", sep=\",\")\n",
    "        continue\n",
    "\n",
    "    max_date = datetime.strptime(max_date[:10], '%Y-%m-%d')\n",
    "    day_after = max_date + timedelta(days=1)\n",
    "    today = datetime.now()\n",
    "    yesterday = today - timedelta(days=1)\n",
    "    if day_after >= yesterday:\n",
    "        print('No need to update ticker {}'.format(ticker))\n",
    "        continue\n",
    "    yahoo_ticker = yf.Ticker(ticker)\n",
    "    data = yahoo_ticker.history(start=day_after.strftime(\"%Y-%m-%d\") , end=yesterday.strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "    data.to_csv(f'data/yahoo_finance/' + ticker + \".csv\", sep=\",\")\n",
    "    for idx, row in data.iterrows():\n",
    "        date = idx.date()\n",
    "        if pd.notna(row['Open']) == False or pd.notna(row['High']) == False or pd.notna(row['Low']) == False or pd.notna(row['Close']) == False or pd.notna(row['Volume']) == False:\n",
    "            print('Skipping data for date {}'.format(date))\n",
    "            continue\n",
    "        open = row['Open']\n",
    "        high = row['High']\n",
    "        low = row['Low']\n",
    "        close = row['Close']\n",
    "        volume = row['Volume']\n",
    "        dividends = row['Dividends']\n",
    "        stocks_split = row['Stock Splits']\n",
    "        c.execute(\"INSERT INTO stocks (date, ticker, open, high, low, close, volume, dividends, stocks_split) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\", \n",
    "            (date.strftime('%Y-%m-%d'), ticker, open, high, low, close, volume, dividends, stocks_split))\n",
    "c.close()\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97624dfa",
   "metadata": {},
   "source": [
    "#### Get diff between companies in S&P 500 list and companies in stocks table and add them to stocks table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6add24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3, os\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "\n",
    "# get all tickers from file\n",
    "df = pd.read_csv('./news/constituents.csv')\n",
    "\n",
    "conn = sqlite3.connect('data/stocks.db')\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT DISTINCT(ticker) FROM stocks ORDER BY ticker')\n",
    "tickers_log = c.fetchall()\n",
    "for index, row in df.iterrows():\n",
    "    ticker_file = row['Symbol'].lower()\n",
    "    found = False\n",
    "    for ticker in tickers_log:\n",
    "        ticker_db = ticker[0]\n",
    "        if ticker_file == ticker_db:\n",
    "            found = True\n",
    "            break\n",
    "    if found == False:\n",
    "        # TODO: make it a function\n",
    "        print('Adding new ticker {}'.format(ticker_file))\n",
    "        yahoo_ticker = yf.Ticker(ticker_file)\n",
    "        data = yahoo_ticker.history(period=\"max\")\n",
    "        for idx, row in data.iterrows():\n",
    "            date = idx.date()\n",
    "            open = row['Open']\n",
    "            high = row['High']\n",
    "            low = row['Low']\n",
    "            close = row['Close']\n",
    "            volume = row['Volume']\n",
    "            dividends = row['Dividends']\n",
    "            stocks_split = row['Stock Splits']\n",
    "            c.execute(\"INSERT INTO stocks (date, ticker, open, high, low, close, volume, dividends, stocks_split) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\", \n",
    "                    (date.strftime(\"%Y-%m-%d\"), ticker_file, open, high, low, close, volume, dividends, stocks_split))\n",
    "        data.to_csv(f'data/yahoo_finance/' + ticker_file + \".csv\", sep=\",\")\n",
    "\n",
    "c.close()\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fb0a0",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0776d5",
   "metadata": {},
   "source": [
    "#### Execute queries with retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e472e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "\n",
    "def execute_with_retry(conn, query, params=(), retries=5, delay=1):\n",
    "\tfor i in range(retries):\n",
    "\t\ttry:\n",
    "\t\t\tc = conn.cursor()\n",
    "\t\t\tc.execute(query, params)\n",
    "\t\t\tconn.commit()\n",
    "\t\t\treturn\n",
    "\t\texcept sqlite3.OperationalError as e:\n",
    "\t\t\tif 'database is locked' in str(e):\n",
    "\t\t\t\ttime.sleep(delay)\n",
    "\t\t\telse:\n",
    "\t\t\t\traise\n",
    "\t\tfinally:\n",
    "\t\t\tc.close()\n",
    "\traise Exception(\"Failed to execute query after multiple retries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c26bf",
   "metadata": {},
   "source": [
    "#### Get all dates of particular day of week in specific date range that are present in stocks table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1821de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_dates_of_day_of_week_in_range(start_date_str, end_date_str, day_of_week):\n",
    "    # get all days of day_of_week for period from start_date to end_date from calendar\n",
    "    dates = []\n",
    "    try:\n",
    "        current_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Invalid date format\", e)\n",
    "    if day_of_week < 0 or day_of_week > 6:\n",
    "        raise ValueError(\"Invalid day of week\", day_of_week)\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        if current_date.weekday() == day_of_week:\n",
    "            dates.append(current_date)        \n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    return dates    \n",
    "\n",
    "def filter_dates_not_in_stock_table(dates):\n",
    "    conn = sqlite3.connect('data/stocks.db', timeout=30)\n",
    "    dates_in_range = []\n",
    "    for date in dates:\n",
    "        c = conn.cursor()\n",
    "        c.execute('SELECT date FROM stocks WHERE date = ? LIMIT 1', (date.strftime(\"%Y-%m-%d\"),))\n",
    "        if c.fetchone():\n",
    "            dates_in_range.append(date)\n",
    "        c.close()    \n",
    "    return dates_in_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bbdfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class TestGetDatesOfDayOfWeek(unittest.TestCase):\n",
    "    def test_one_week_range(self):\n",
    "        # Test for a single week range\n",
    "        result = get_dates_of_day_of_week_in_range(\"2024-01-01\", \"2024-01-07\", 0)  # Monday\n",
    "        self.assertEqual(len(result), 1)\n",
    "        self.assertEqual(result[0].strftime(\"%Y-%m-%d\"), \"2024-01-01\")\n",
    "\n",
    "    def test_month_range(self):\n",
    "        # Test for a month range\n",
    "        result = get_dates_of_day_of_week_in_range(\"2024-01-01\", \"2024-01-31\", 2)  # Wednesday\n",
    "        self.assertEqual(len(result), 5)  # Should have 5 Wednesdays in January 2024\n",
    "        self.assertTrue(all(d.weekday() == 2 for d in result))\n",
    "\n",
    "    def test_empty_range(self):\n",
    "        # Test when end date is before start date\n",
    "        result = get_dates_of_day_of_week_in_range(\"2024-01-31\", \"2024-01-01\", 1)\n",
    "        self.assertEqual(len(result), 0)\n",
    "\n",
    "    def test_same_day(self):\n",
    "        # Test when start and end date are the same\n",
    "        result = get_dates_of_day_of_week_in_range(\"2024-01-01\", \"2024-01-01\", 0)  # Monday\n",
    "        self.assertEqual(len(result), 1)\n",
    "        result = get_dates_of_day_of_week_in_range(\"2024-01-01\", \"2024-01-01\", 1)  # Tuesday\n",
    "        self.assertEqual(len(result), 0)\n",
    "\n",
    "    def test_invalid_day_of_week(self):\n",
    "        # Test with invalid day of week values\n",
    "        with self.assertRaises(ValueError):\n",
    "            get_dates_of_day_of_week_in_range(\"2024-01-01\", \"2024-01-07\", 7)\n",
    "        with self.assertRaises(ValueError):\n",
    "            get_dates_of_day_of_week_in_range(\"2024-01-01\", \"2024-01-07\", -1)\n",
    "\n",
    "    def test_invalid_date_format(self):\n",
    "        # Test with invalid date format\n",
    "        with self.assertRaises(ValueError):\n",
    "            get_dates_of_day_of_week_in_range(\"2024/01/01\", \"2024-01-07\", 0)\n",
    "        with self.assertRaises(ValueError):\n",
    "            get_dates_of_day_of_week_in_range(\"2024-01-01\", \"invalid_date\", 0)\n",
    "\n",
    "    def test_leap_year(self):\n",
    "        # Test handling of leap year\n",
    "        result = get_dates_of_day_of_week_in_range(\"2024-02-01\", \"2024-02-29\", 3)  # Thursday\n",
    "        self.assertEqual(len(result), 5)\n",
    "        self.assertTrue(all(d.weekday() == 3 for d in result))\n",
    "\n",
    "    def test_all_days_in_week(self):\n",
    "        # Test for all days in a week\n",
    "        start_date = \"2024-01-01\"\n",
    "        end_date = \"2024-01-07\"\n",
    "        total_days = sum(len(get_dates_of_day_of_week_in_range(start_date, end_date, day)) \n",
    "                        for day in range(7))\n",
    "        self.assertEqual(total_days, 7)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('bigdata/one/logs.db')\n",
    "c = conn.cursor()\n",
    "# get unique indicators names\n",
    "c.execute('SELECT distinct(ticker) FROM log_models')\n",
    "fetch = c.fetchall()\n",
    "tickers_log = [ticker[0] for ticker in fetch]\n",
    "\n",
    "print(\"Total number of tickers: \", len(tickers_log))\n",
    "c.close()\n",
    "conn.close()\n",
    "\n",
    "conn = sqlite3.connect('data/stocks.db')\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT distinct(ticker) FROM stocks')\n",
    "fetch = c.fetchall()\n",
    "tickers_stocks = [ticker[0] for ticker in fetch]\n",
    "print(\"Total number of tickers: \", len(tickers_stocks))\n",
    "c.close()\n",
    "conn.close()\n",
    "\n",
    "for ticker in tickers_stocks:\n",
    "    if ticker not in tickers_log:\n",
    "        print(\"{}\".format(ticker))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd720667",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db66be1b",
   "metadata": {},
   "source": [
    "#### Reset news table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24b43df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('data/stocks.db', timeout=30)\n",
    "try:\n",
    "\texecute_with_retry(conn, \"DROP TABLE IF EXISTS news\")\n",
    "\texecute_with_retry(conn, \"VACUUM\")\n",
    "\texecute_with_retry(conn, \"CREATE TABLE EXISTS news (date text, ticker text, json_news text, avg_sentiment real) STRICT\")\n",
    "\texecute_with_retry(conn, 'CREATE UNIQUE INDEX idx_unique_news_ticker_date ON news(ticker,date)')\n",
    "finally:\n",
    "\tif conn:\n",
    "\t\tconn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6112c",
   "metadata": {},
   "source": [
    "#### Average news sentiment for the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b15fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_news_sentiment(ticker, news) -> float:\n",
    "    try:\n",
    "        if news['items'] == 0:\n",
    "            return 0.0\n",
    "        if len(news['feed']) == 0:\n",
    "            return 0.0\n",
    "    except Exception as e:\n",
    "        print(\"Ticker\",ticker,\"News json\", news)\n",
    "        return 0.0    \n",
    "    scores_arr = []\n",
    "    for item in news['feed']:\n",
    "        for sentiment in item['ticker_sentiment']:\n",
    "            if sentiment['ticker'].lower() == ticker.lower():\n",
    "                scores_arr.append(float(sentiment['ticker_sentiment_score'])*float(sentiment['relevance_score']))\n",
    "    if len(scores_arr) == 0:\n",
    "        return 0.0\n",
    "    return sum(scores_arr)/len(scores_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f4ad4",
   "metadata": {},
   "source": [
    "#### Fill up news table with news from news/data/{company}/{date}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81388242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:30:51.948266Z",
     "start_time": "2024-10-06T18:26:45.201060Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3, os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "conn = sqlite3.connect('data/stocks.db')\n",
    "for company_news_dir in os.listdir('./news/data/'):\n",
    "    if os.path.isdir('./news/data/' + company_news_dir) == False:\n",
    "        continue\n",
    "    for model_name in os.listdir('./news/data/' + company_news_dir):\n",
    "        if model_name.endswith('.json') == False:\n",
    "            continue\n",
    "        try:\n",
    "            with open('./news/data/'+company_news_dir+'/' + model_name, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "                if 'items' in json_data:\n",
    "                    if json_data['items'] == '0':\n",
    "                        file.close()\n",
    "                        continue\n",
    "                file.close()\n",
    "        except Exception as e:\n",
    "            print('Error reading file {}, error: {}'.format(model_name, e))\n",
    "            continue\n",
    "        \n",
    "        ticker = company_news_dir\n",
    "        date = datetime.strptime(model_name[:8], \"%Y%m%d\")\n",
    "        avg_sentiment = average_news_sentiment(ticker, json_data)\n",
    "        json_news = json.dumps(json_data)\n",
    "        if avg_sentiment == 0.0:\n",
    "            continue\n",
    "        execute_with_retry(conn, \"INSERT OR REPLACE INTO news (date, ticker, json_news, avg_sentiment) VALUES (?, ?, ?, ?)\",\n",
    "                    (date.strftime(\"%Y-%m-%d\"), ticker, json_news, avg_sentiment), retries=2, delay=1)        \n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e29b4c",
   "metadata": {},
   "source": [
    "#### Clean up invalid news files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "removed_files = 0\n",
    "bad_arr = []\n",
    "for company_news_dir in os.listdir('./news/data/'):\n",
    "    if os.path.isdir('./news/data/' + company_news_dir) == False:\n",
    "        continue\n",
    "    amount_of_bad_files = 0\n",
    "    for model_name in os.listdir('./news/data/' + company_news_dir):\n",
    "        try:\n",
    "            with open('./news/data/' + company_news_dir + '/' + model_name, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "                if 'items' not in json_data:\n",
    "                    os.remove('./news/data/' + company_news_dir + '/' + filename)\n",
    "                    removed_files += 1\n",
    "                    amount_of_bad_files += 1\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print('Error reading file {}, error: {}'.format(model_name, e))\n",
    "            continue\n",
    "    if amount_of_bad_files > 0:\n",
    "        bad_arr.append((company_news_dir, amount_of_bad_files))\n",
    "print(f'Removed {removed_files} files')\n",
    "print(bad_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98feba5e",
   "metadata": {},
   "source": [
    "#### Get records with zero news sentiment (expected to be empty) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c6d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3, os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "conn = sqlite3.connect('data/stocks.db')\n",
    "c = conn.cursor()\n",
    "c.execute(\"SELECT ticker, date FROM news where avg_sentiment = 0.0\")\n",
    "data = c.fetchall()\n",
    "\n",
    "print(\"Tickers with 0.0 sentiment\")\n",
    "for row in data:\n",
    "    ticker = row[0]\n",
    "    date = row[1]\n",
    "    print(\"Ticker {} for date {}\".format(ticker, date))\n",
    "c.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235ad12",
   "metadata": {},
   "source": [
    "#### Get latest news from news files and update news table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d3798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, os\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "conn = sqlite3.connect('data/stocks.db')\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT DISTINCT(ticker) FROM news ORDER BY ticker')\n",
    "tickers_log = c.fetchall()\n",
    "c.close()\n",
    "\n",
    "for ticker in tickers_log:\n",
    "    ticker = ticker[0]\n",
    "    # print('Processing ticker {}'.format(ticker))\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT max(date) FROM news WHERE ticker = ?', (ticker,))\n",
    "    max_date = c.fetchone()[0]\n",
    "    c.close()\n",
    "    if max_date is None:\n",
    "        print('Need to get the whole history for this ticker {}'.format(ticker))\n",
    "        continue\n",
    "    max_date = datetime.strptime(max_date[:10], '%Y-%m-%d')\n",
    "    day_after = max_date + timedelta(days=1)\n",
    "    today = datetime.now()\n",
    "    yesterday = today - timedelta(days=1)\n",
    "    if day_after > yesterday:\n",
    "        # print('No need to update ticker {}'.format(ticker))\n",
    "        continue\n",
    "    for company_news_dir in os.listdir('./news/data/'):\n",
    "        if company_news_dir != ticker:\n",
    "            continue\n",
    "        if os.path.isdir('./news/data/' + company_news_dir) == False:\n",
    "            continue\n",
    "        \n",
    "        for model_name in os.listdir('./news/data/' + company_news_dir):\n",
    "            if model_name.endswith('.json') == False:\n",
    "                continue\n",
    "            date = datetime.strptime(model_name[:8], \"%Y%m%d\")\n",
    "            if date <= max_date:\n",
    "                continue\n",
    "            with open('./news/data/'+company_news_dir+'/' + model_name, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "                if 'items' in json_data:\n",
    "                    if json_data['items'] == '0':\n",
    "                        file.close()\n",
    "                        continue\n",
    "                file.close()\n",
    "            json_news = json.dumps(json_data)\n",
    "            print('Adding news for {} for date {}'.format(ticker, date.strftime('%Y-%m-%d')))\n",
    "            avg_sentiment = average_news_sentiment(ticker, json_data)\n",
    "            execute_with_retry(conn, \"INSERT INTO news (date, ticker, json_news, avg_sentiment) VALUES (?, ?, ?, ?)\",\n",
    "                        (date.strftime(\"%Y-%m-%d\"), ticker, json_news, avg_sentiment), retries=2, delay=1)\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0644b7",
   "metadata": {},
   "source": [
    "#### Create unique index in table news for columns ticker and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9fb23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('data/stocks.db')\n",
    "c = conn.cursor()\n",
    "try:\n",
    "    c.execute('CREATE UNIQUE INDEX idx_unique_news_ticker_date ON news(ticker,date)')\n",
    "except Exception as e:\n",
    "    print(\"create unique index for news table: \",e)    \n",
    "\n",
    "c.close()\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8daae",
   "metadata": {},
   "source": [
    "#### Total number of news in news table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3, os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sources = {}\n",
    "conn = sqlite3.connect('data/stocks.db')\n",
    "c = conn.cursor()\n",
    "c.execute(\"SELECT * FROM news where date >= '2022-03-01' and date <= '2024-11-01' and ticker in ('aapl')\")\n",
    "data = c.fetchall()\n",
    "c.close()\n",
    "conn.close()\n",
    "import json\n",
    "total_items = 0\n",
    "skipped_items = 0\n",
    "for row in data:\n",
    "    df = json.loads(row[2])\n",
    "    if 'items' in df:\n",
    "        if df['items'] == '0':\n",
    "            continue\n",
    "        if 'feed' in df:\n",
    "            if len(df['feed']) == 0:\n",
    "                continue\n",
    "            for item in df['feed']:\n",
    "                if 'source' in item:\n",
    "                    if len(item['source']) == 0:\n",
    "                        continue\n",
    "                if item['source'] not in sources:\n",
    "                    sources[item['source']] = 1\n",
    "                else:\n",
    "                    sources[item['source']] += 1\n",
    "        total_items += int(df['items'])\n",
    "    else:\n",
    "        skipped_items += 1\n",
    "print(total_items)\n",
    "print(skipped_items)\n",
    "print(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a9128",
   "metadata": {},
   "source": [
    "#### Get news from alphavantage API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b69c8a5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T16:58:38.772032Z",
     "start_time": "2024-10-06T16:50:40.710438Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import sqlite3\n",
    "\n",
    "START_DATE_STR = \"01032022\"\n",
    "LIMIT = 1000\n",
    "start_time = time.time()\n",
    "# GET https://www.alphavantage.co/query?apikey=NS9LMM2DJBDQC1W7&function=NEWS_SENTIMENT&sort=EARLIEST&tickers=MSFT&time_from=20220302T0130&time_to=20220303T0130&limit=1000\n",
    "\n",
    "# Set the API endpoint and parameters\n",
    "ENDPOINT = \"https://www.alphavantage.co/query\"\n",
    "PREMIUM_APIKEY = \"**************\"\n",
    "ALPHA_FUNCTION = \"NEWS_SENTIMENT\"\n",
    "\n",
    "LOG_FILE = \"./news.log\"\n",
    "# Truncate the log file\n",
    "with open(LOG_FILE, \"w\") as file:\n",
    "    file.write(\"\")\n",
    "    file.close()\n",
    "\n",
    "conn = sqlite3.connect('data/stocks.db')\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT DISTINCT(ticker) FROM stocks ORDER BY ticker')\n",
    "tickers_log = c.fetchall()\n",
    "conn.close()\n",
    "\n",
    "total_counter = 0\n",
    "ticker_counter = 0\n",
    "local_ticker_counter = 0\n",
    "\n",
    "for ticker in tickers_log:\n",
    "    ticker = ticker[0]\n",
    "    os.makedirs('./news/data/' + ticker, exist_ok=True) \n",
    "\n",
    "    ticker_counter += 1\n",
    "    local_ticker_counter = 0\n",
    "\n",
    "    # Get the current date\n",
    "    current_date = datetime.now().date()\n",
    "\n",
    "    # Loop through each day from a specific start date to the current date\n",
    "    start_date = datetime.strptime(START_DATE_STR, \"%d%m%Y\").date()\n",
    "    delta = timedelta(days=1)\n",
    "\n",
    "    while start_date < current_date:\n",
    "        local_ticker_counter += 1\n",
    "        total_counter += 1\n",
    "        # Format the date as YYYY-MM-DD\n",
    "        date_from_str = start_date.strftime(\"%Y%m%d\")\n",
    "        date_to_str = (start_date + delta).strftime(\"%Y%m%d\")\n",
    "\n",
    "        # Set the time range for the API request\n",
    "        time_from = f\"{date_from_str}T0000\"\n",
    "        time_to = f\"{date_to_str}T0000\"\n",
    "\n",
    "        # Continue if the data has already been retrieved\n",
    "        model_name = f\"./news/data/{ticker}/{date_from_str}.json\"\n",
    "        if os.path.exists(model_name):\n",
    "            # with open(LOG_FILE, \"a\") as file:\n",
    "                # file.write(f\"{ticker_counter:<4} {local_ticker_counter:<4} {total_counter:<7} Data for {date_from_str} already exists\\n\")\n",
    "                # file.close()\n",
    "            start_date += delta\n",
    "            continue\n",
    "\n",
    "        time.sleep(2)\n",
    "        # Construct the API request URL\n",
    "        url = f\"{ENDPOINT}?apikey={PREMIUM_APIKEY}&function={ALPHA_FUNCTION}&sort=EARLIEST&tickers={ticker}&time_from={time_from}&time_to={time_to}&limit={LIMIT}\"\n",
    "\n",
    "        # Send the API request\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raise an exception if the request was not successful\n",
    "            if response.status_code == 200:\n",
    "                # Parse the JSON response\n",
    "                data = response.json()\n",
    "\n",
    "                # Save the JSON data to a file\n",
    "                # filename = f\"./news/data/{ticker}/{date_from_str}.json\"\n",
    "                with open(model_name, \"w\") as file:\n",
    "                    json.dump(data, file)\n",
    "                    file.close()\n",
    "\n",
    "                with open(LOG_FILE, \"a\") as file:\n",
    "                    file.write(f\"{ticker_counter:<4} {local_ticker_counter:<4} {total_counter:<7} Saved data for {date_from_str} to {model_name}\\n\")\n",
    "                    file.close()\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data for {date_from_str}\")\n",
    "\n",
    "            # Move to the next day\n",
    "            start_date += delta\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            with open(LOG_FILE, \"a\") as file:\n",
    "                file.write(f\"An error occurred: {e}. Perform request again\\n\")\n",
    "                file.close()\n",
    "            # Handle the error here        \n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time\n",
    "execution_time = end_time - start_time\n",
    "with open(LOG_FILE, \"a\") as file:\n",
    "    file.write(f\"Execution time: {execution_time} seconds\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e290374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# replace the \"demo\" apikey below with your own key from https://www.alphavantage.co/support/#api-key\n",
    "url = 'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=CE&apikey=UQCN3VITQV0UWTZD'\n",
    "r = requests.get(url)\n",
    "data = r.json()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cdb258",
   "metadata": {},
   "source": [
    "#### Get news for specific ticker and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f5af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import sqlite3\n",
    "\n",
    "START_DATE_STR = \"01122021\"\n",
    "LIMIT = 200\n",
    "# GET https://www.alphavantage.co/query?apikey=NS9LMM2DJBDQC1W7&function=NEWS_SENTIMENT&sort=EARLIEST&tickers=MSFT&time_from=20220302T0130&time_to=20220303T0130&limit=1000\n",
    "\n",
    "# Set the API endpoint and parameters\n",
    "ENDPOINT = \"https://www.alphavantage.co/query\"\n",
    "PREMIUM_APIKEY = \"**************\"\n",
    "ALPHA_FUNCTION = \"NEWS_SENTIMENT\"\n",
    "\n",
    "LOG_FILE = \"./news.log\"\n",
    "# Truncate the log file\n",
    "with open(LOG_FILE, \"w\") as file:\n",
    "    file.write(\"\")\n",
    "    file.close()\n",
    "start_date = datetime.strptime(START_DATE_STR, \"%d%m%Y\").date()\n",
    "delta = timedelta(days=1)\n",
    "date_from_str = start_date.strftime(\"%Y%m%d\")\n",
    "date_to_str = (start_date + delta).strftime(\"%Y%m%d\")\n",
    "\n",
    "# Set the time range for the API request\n",
    "time_from = f\"{date_from_str}T0000\"\n",
    "time_to = f\"{date_to_str}T0000\"\n",
    "ticker = 'nvda'\n",
    "\n",
    "\n",
    "\n",
    "url = f\"{ENDPOINT}?apikey={PREMIUM_APIKEY}&function={ALPHA_FUNCTION}&sort=EARLIEST&tickers={ticker}&time_from={time_from}&time_to={time_to}&limit={LIMIT}\"\n",
    "\n",
    "# Send the API request\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception if the request was not successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        print(data)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {date_from_str}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31076eb0",
   "metadata": {},
   "source": [
    "#### Get data from eodhd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb58e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eodhd import APIClient\n",
    "\n",
    "api = APIClient(api_key=\"674210e833d2f0.82915138\")\n",
    "print(api.get_sentiment(\"aapl.us\", from_date=\"2014-01-01\", to_date=\"2018-01-01\"))\n",
    "# print(api.financial_news(s=\"aapl\", from_date=\"2018-01-01\", to_date=\"2018-02-22\"))\n",
    "\n",
    "# import requests\n",
    "\n",
    "# url = f'https://eodhd.com/api/sentiments?s=btc-usd.cc,aapl.us&from=2022-01-01&to=2022-04-22&api_token=674210e833d2f0.82915138&fmt=json'\n",
    "# data = requests.get(url).json()\n",
    "\n",
    "# print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd2c4d",
   "metadata": {},
   "source": [
    "#### Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb1643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jPJVdNzmDVFgubp7IHptXPjBqko0o8nx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a14114",
   "metadata": {},
   "source": [
    "TODO: make sctipt to clean up news for companies that are no longer in S&P 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8196fd1",
   "metadata": {},
   "source": [
    "### Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7615f6",
   "metadata": {},
   "source": [
    "#### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "296854ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = ['200_MA', 'AARON', 'MACD', 'OBV', 'RSI', 'ADX', 'AD']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d34ec",
   "metadata": {},
   "source": [
    "#### Reset indicators table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "446672f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('data/stocks.db', timeout=30)\n",
    "try:\n",
    "\texecute_with_retry(conn, 'DROP INDEX IF EXISTS idx_unique_indicator_ticker_date')\n",
    "\texecute_with_retry(conn, \"DROP TABLE IF EXISTS indicator\")\n",
    "\texecute_with_retry(conn, \"VACUUM\")\n",
    "\texecute_with_retry(conn, \"CREATE TABLE indicator (date text, ticker text, indicator text, value real) STRICT\")\n",
    "\texecute_with_retry(conn, 'CREATE UNIQUE INDEX idx_unique_indicator_ticker_date_indicator ON indicator(ticker,date,indicator)')\n",
    "finally:\n",
    "\tif conn:\n",
    "\t\tconn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed5a0e",
   "metadata": {},
   "source": [
    "#### Calculate indicators for the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89ddacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_indicators(ticker, data_dir):\n",
    "    try:\n",
    "        df = pd.read_csv(data_dir+f'/{ticker}.csv')\n",
    "    except Exception as e:\n",
    "        print(\"Error reading file: \", e)\n",
    "        return None\n",
    "    for indicator in indicators:\n",
    "        if indicator == '200_MA':\n",
    "            df[indicator] = df['Close'].rolling(window=200).mean()\n",
    "            df[indicator] = df[indicator].fillna(0)\n",
    "            continue\n",
    "        if indicator == 'AARON':\n",
    "            # calculate the Aroon Oscillator for the dataset for the last 25 days\n",
    "            df['Aroon_Up'] = df['High'].rolling(window=25).apply(lambda x: (25 - np.argmax(x)) / 25)\n",
    "            df['Aroon_Down'] = df['Low'].rolling(window=25).apply(lambda x: (25 - np.argmin(x)) / 25)\n",
    "            df[indicator] = df['Aroon_Up'] - df['Aroon_Down']\n",
    "            continue\n",
    "        if indicator == 'MACD':\n",
    "            # calculate the MACD for the dataset\n",
    "            df['26_EMA'] = df['Close'].ewm(span=26).mean()\n",
    "            df['12_EMA'] = df['Close'].ewm(span=12).mean()\n",
    "            df[indicator] = df['12_EMA'] - df['26_EMA']\n",
    "            continue\n",
    "        if indicator == 'OBV':\n",
    "            # calculate the On Balance Volume for the dataset\n",
    "            df[indicator] = np.where(df['Close'] > df['Close'].shift(1), df['Volume'], np.where(df['Close'] < df['Close'].shift(1), -df['Volume'], 0)).cumsum()\n",
    "            continue\n",
    "        if indicator == 'RSI':\n",
    "            # calculate the Relative Strength Index for the dataset\n",
    "            delta = df['Close'].diff()\n",
    "            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "            rs = gain / loss\n",
    "            df[indicator] = 100 - (100 / (1 + rs))\n",
    "            continue\n",
    "        if indicator == 'ADX':\n",
    "            # calculate the Average Directional Index for the dataset\n",
    "            df['High-Low'] = df['High'] - df['Low']\n",
    "            df['High-PreviousClose'] = np.abs(df['High'] - df['Close'].shift(1))\n",
    "            df['Low-PreviousClose'] = np.abs(df['Low'] - df['Close'].shift(1))\n",
    "            df['TR'] = df[['High-Low', 'High-PreviousClose', 'Low-PreviousClose']].max(axis=1)\n",
    "            df['+DM'] = np.where((df['High'] - df['High'].shift(1)) > (df['Low'].shift(1) - df['Low']), df['High'] - df['High'].shift(1), 0)\n",
    "            df['-DM'] = np.where((df['Low'].shift(1) - df['Low']) > (df['High'] - df['High'].shift(1)), df['Low'].shift(1) - df['Low'], 0)\n",
    "            df['+DM1'] = np.where(df['+DM'] > df['-DM'], df['+DM'], 0)\n",
    "            df['-DM1'] = np.where(df['+DM'] < df['-DM'], df['-DM'], 0)\n",
    "            df['TR14'] = df['TR'].rolling(window=14).sum()\n",
    "            df['+DM14'] = df['+DM1'].rolling(window=14).sum()\n",
    "            df['-DM14'] = df['-DM1'].rolling(window=14).sum()\n",
    "            df['+DI14'] = (df['+DM14'] / df['TR14']) * 100\n",
    "            df['-DI14'] = (df['-DM14'] / df['TR14']) * 100\n",
    "            df['DI_diff'] = np.abs(df['+DI14'] - df['-DI14'])\n",
    "            df['DI_sum'] = df['+DI14'] + df['-DI14']\n",
    "            df[indicator] = (df['DI_diff'] / df['DI_sum']) * 100\n",
    "            continue\n",
    "        if indicator == 'AD':\n",
    "            # calculate the Accumulation/Distribution for the dataset\n",
    "            df['MFM'] = ((df['Close'] - df['Low']) - (df['High'] - df['Close'])) / (df['High'] - df['Low'])\n",
    "            df['MFM'].fillna(0)\n",
    "            df['MFV'] = df['MFM'] * df['Volume']\n",
    "            df[indicator] = df['MFV'].cumsum()\n",
    "            continue\n",
    "        else:\n",
    "            df[indicator] = df[indicator].fillna(0)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd178852",
   "metadata": {},
   "source": [
    "#### Write indicators to indicators table for the comapny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1855590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, os\n",
    "from datetime import datetime\n",
    "try:\n",
    "    conn = sqlite3.connect('data/stocks.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT DISTINCT(ticker) FROM stocks ORDER BY ticker')\n",
    "    tickers_log = c.fetchall()\n",
    "    c.close()\n",
    "    for ticker in tickers_log:\n",
    "        ticker = ticker[0]\n",
    "        print('Processing ticker {}'.format(ticker))\n",
    "        df = get_indicators(ticker, 'data/20241209')\n",
    "        if df is None:\n",
    "            print('No data for ticker {}'.format(ticker))\n",
    "            continue\n",
    "        for index, row in df.iterrows():\n",
    "            date = row['Date'][:10]\n",
    "            for indicator in indicators:\n",
    "                value = row[indicator]\n",
    "                if value is None or value == 0.0:\n",
    "                    continue\n",
    "                execute_with_retry(\n",
    "                                    conn,\n",
    "                                    \"INSERT OR REPLACE INTO indicator (date, ticker, indicator, value) VALUES (?, ?, ?, ?)\", \n",
    "                                    (date, ticker, indicator, value))\n",
    "finally:\n",
    "\tif conn:\n",
    "\t\tconn.close()                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0614584",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddda279",
   "metadata": {},
   "source": [
    "#### Generate train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e86f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Generate training data with a sliding window of size seq_len.\n",
    "# Return X_train and Y_train\n",
    "def generate_train_data(data,  seq_len=10,up_front_prediction_days=7, tech_indicator=\"200_MA\", indicators=[], no_news=False, no_indicators=False):\n",
    "    features = ['volume','close']\n",
    "    if not no_news:\n",
    "        features = ['volume','close','avg_sentiment']\n",
    "    if not no_indicators:\n",
    "        features.append(tech_indicator)\n",
    "    target = 'close'\n",
    "\n",
    "    # Prepare the dataset\n",
    "    X = data[features]\n",
    "    Y = data[target]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    Y_scaled = scaler.fit_transform(pd.DataFrame(Y))  # Convert Y to DataFrame\n",
    "\n",
    "    X = X_scaled\n",
    "    Y = Y_scaled\n",
    "\n",
    "    split = int(0.8 * len(X_scaled))\n",
    "    x_train, x_test = X_scaled[:split], X_scaled[split:]\n",
    "    y_train, y_test = Y_scaled[:split], Y_scaled[split:]\n",
    "\n",
    "    x_train, x_test = pd.DataFrame(x_train), pd.DataFrame(x_test)\n",
    "    y_train, y_test = pd.DataFrame(y_train), pd.DataFrame(y_test)\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "\n",
    "    # Calculate the 200-day moving average\n",
    "    # if self.tech_indicator == '200_MA':\n",
    "        # X['200_MA'] = X['Close'].rolling(window=200).mean()\n",
    "    # Based on them construct technical indicators: \n",
    "    # OBV, A/D, Average directional index, Aroon oscillator, MACD, RSI, Stochastic oscillator, SMA\n",
    "        \n",
    "\n",
    "    # TODO: Add VIX (Volatility Index) to the dataset\n",
    "    \n",
    "    for i in range((len(x_train)//seq_len)*seq_len - seq_len - up_front_prediction_days):\n",
    "        x = np.array(x_train.iloc[i: i + seq_len])\n",
    "        y = np.array([y_train.iloc[i + seq_len + up_front_prediction_days]], np.float64)\n",
    "        X_train.append(x)\n",
    "        Y_train.append(y)\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = np.array(Y_train)\n",
    "\n",
    "    for i in range((len(x_test)//seq_len)*seq_len - seq_len - up_front_prediction_days):\n",
    "        x = np.array(x_test.iloc[i: i + seq_len])\n",
    "        y = np.array(y_test.iloc[i + seq_len + up_front_prediction_days], np.float64)\n",
    "        X_test.append(x)\n",
    "        Y_test.append(y)\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bb1307",
   "metadata": {},
   "source": [
    "#### Generate inference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c7b0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def stock_data_to_df(ticker,date_to_predict, seq_len=35, up_front_prediction_days=5, tech_indicator=\"200_MA\",  no_news=False, no_indicators=False):\n",
    "    conn = sqlite3.connect('data/stocks.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT date, close, volume FROM stocks WHERE ticker = ? and date <= ? ORDER BY date DESC LIMIT ?', (ticker,date_to_predict.strftime(\"%Y-%m-%d\"),seq_len + up_front_prediction_days))\n",
    "    prices = c.fetchall()\n",
    "    data = pd.DataFrame(prices, columns=['date', 'close', 'volume'])\n",
    "    c.close()\n",
    "    # reverse data\n",
    "    data = data.iloc[::-1]\n",
    "    min_date = data.iloc[0]['date']\n",
    "    max_date = data.iloc[-1]['date']\n",
    "    if not no_news: \n",
    "        c = conn.cursor()\n",
    "        c.execute('SELECT date, avg_sentiment FROM news WHERE ticker = ? and date >= ? and date <= ?', (ticker,min_date,max_date ))\n",
    "        news_sentiment = c.fetchall()\n",
    "        c.close()\n",
    "        data = pd.merge(data, pd.DataFrame(news_sentiment, columns=['date', 'avg_sentiment']), on='date', how='left')\n",
    "        # Fill NaN values in avg_sentiment with 0\n",
    "        data['avg_sentiment'] = data['avg_sentiment'].fillna(0)\n",
    "\n",
    "    if not no_indicators:\n",
    "        c = conn.cursor()\n",
    "        c.execute('SELECT date, value FROM indicator WHERE ticker = ? and date <= ? and indicator = ? ORDER BY date DESC LIMIT ?', (ticker,date_to_predict.strftime(\"%Y-%m-%d\"),tech_indicator,seq_len + up_front_prediction_days ))\n",
    "        tech_indicators = c.fetchall()\n",
    "        c.close()\n",
    "        data = pd.merge(data, pd.DataFrame(tech_indicators, columns=['date', tech_indicator]), on='date', how='left')\n",
    "\n",
    "    predicted_stock_price = data.iloc[-1]['close']\n",
    "    data = data.iloc[:-up_front_prediction_days]\n",
    "    return data, predicted_stock_price\n",
    "\n",
    "\n",
    "# Generate training data with a sliding window of size seq_len.\n",
    "# Return X_train and Y_train\n",
    "def generate_inference_data(data, tech_indicator=\"200_MA\", no_news=False, no_indicators=False):\n",
    "    features = ['volume','close']\n",
    "    if not no_news:\n",
    "        features = ['volume','close','avg_sentiment']\n",
    "    if not no_indicators:\n",
    "        features.append(tech_indicator)\n",
    "\n",
    "    # Prepare the dataset\n",
    "    X = data[features]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_reshaped = X_scaled.reshape(1, X_scaled.shape[0], X_scaled.shape[1])\n",
    "\n",
    "\n",
    "    return X_reshaped, scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13bd545",
   "metadata": {},
   "source": [
    "#### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4d4771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def build_model(seq_len, amount_of_params, optimizer, loss_function, activation, amount_of_neurons):\n",
    "    model = Sequential()\n",
    "    # Adding the first LSTM layer\n",
    "    model.add(\n",
    "        keras.layers.Input(shape=(seq_len, amount_of_params)),\n",
    "        keras.layers.LSTM(amount_of_neurons, return_sequences=True,\n",
    "                        activation=activation))\n",
    "    model.add(keras.layers.LSTM(64, return_sequences=True,  activation=activation))\n",
    "    model.add(keras.layers.LSTM(40, activation=activation))\n",
    "    model.add(keras.layers.Dense(1, activation=activation))\n",
    "    # Compiling the RNN\n",
    "    model.compile(optimizer=optimizer, loss=loss_function)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c8f79",
   "metadata": {},
   "source": [
    "#### Log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9040e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FILE = './logs/model.log'\n",
    "def output_in_logfile(message):\n",
    "    with open(LOG_FILE, \"a\") as file:\n",
    "        file.write(message+'\\n')\n",
    "        file.close()\n",
    "\n",
    "#truncate log file\n",
    "with open(LOG_FILE, \"w\") as file:\n",
    "    file.write(\"\")\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc5352f",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd017a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.models import Sequential\n",
    "from datetime import timedelta\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "\n",
    "SEQ_LEN = 35\n",
    "AMOUNT_OF_PARAMS = 4\n",
    "UP_FRONT_PREDICTION_DAYS = 5\n",
    "AMOUNT_OF_NEURONS = 128\n",
    "# OPTIMIZERS = ['adam', 'sgd', 'rmsprop', 'adagrad', 'adamax', 'nadam']\n",
    "OPTIMIZERS = ['adam', 'rmsprop', 'nadam']\n",
    "OPTIMIZER = OPTIMIZERS[2]\n",
    "LOSS_FUNCTIONS = ['mean_squared_error', 'huber', 'log_cosh']\n",
    "LOSS_FUNCTION = LOSS_FUNCTIONS[1]\n",
    "# ACTIVATIONS = ['relu', 'tanh','leaky_relu','elu','selu']\n",
    "ACTIVATIONS = ['relu', 'tanh','leaky_relu','elu']\n",
    "ACTIVATION = ACTIVATIONS[1]\n",
    "COMPANIES_CSV_PATH = './news/constituents.csv'\n",
    "\n",
    "# STOCH, SO - ?\n",
    "TECH_INDICATORS = ['RSI', 'OBV', 'ADX', 'MACD', 'AD', 'AARON','200_MA']\n",
    "TECH_INDICATOR = TECH_INDICATORS[0]\n",
    "NUM_THREADS = 4\n",
    "WITHOUT_NEWS = True\n",
    "WITHOUT_INDICATORS = False\n",
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "conn = sqlite3.connect('data/stocks.db')\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT DISTINCT(ticker) FROM stocks ORDER BY ticker')\n",
    "tickers_log = c.fetchall()\n",
    "c.close()\n",
    "\n",
    "# date one year ago\n",
    "DATE_ONE_YEAR_AGO = (datetime.now() - timedelta(days=365)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "for ticker in tickers_log:\n",
    "    ticker = ticker[0]\n",
    "    if ticker != 'nvda':\n",
    "        continue\n",
    "\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT date, close, volume FROM stocks WHERE ticker = ? and date >= \"2022-03-01\" and date <= ? ORDER BY date', (ticker,DATE_ONE_YEAR_AGO))\n",
    "    prices = c.fetchall()\n",
    "    c.close()\n",
    "    print(f'Prices: {len(prices)}')\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT date, avg_sentiment FROM news WHERE ticker = ? and date <= ? ORDER BY date', (ticker,DATE_ONE_YEAR_AGO))\n",
    "    news_sentiment = c.fetchall()\n",
    "    c.close()\n",
    "\n",
    "    # [('200_MA',), ('AARON',), ('AD',), ('ADX',), ('MACD',), ('OBV',), ('RSI',)]\n",
    "    query = f'SELECT date, value as \"{TECH_INDICATOR}\" FROM indicator WHERE ticker = ? and date >= \"2022-03-01\" and date <= ? and indicator = ? ORDER BY date'\n",
    "    c = conn.cursor()\n",
    "    c.execute(query, (ticker, DATE_ONE_YEAR_AGO, TECH_INDICATOR))\n",
    "    tech_indicators = c.fetchall()\n",
    "    c.close()\n",
    "\n",
    "\n",
    "    combined_data = pd.DataFrame(prices, columns=['date', 'close', 'volume'])\n",
    "    if not WITHOUT_NEWS:\n",
    "        combined_data = pd.merge(combined_data, pd.DataFrame(news_sentiment, columns=['date', 'avg_sentiment']), on='date', how='left')\n",
    "        # Fill NaN values in avg_sentiment with 0\n",
    "        combined_data['avg_sentiment'] = combined_data['avg_sentiment'].fillna(0)\n",
    "    else:\n",
    "        AMOUNT_OF_PARAMS -= 1\n",
    "    if not WITHOUT_INDICATORS:\n",
    "        combined_data = pd.merge(combined_data, pd.DataFrame(tech_indicators, columns=['date', TECH_INDICATOR]), on='date', how='left')\n",
    "    else:\n",
    "        AMOUNT_OF_PARAMS -= 1 \n",
    "\n",
    "    len_before = len(combined_data)\n",
    "    # remove rows with NaN values\n",
    "    combined_data = combined_data.dropna()\n",
    "    len_after = len(combined_data)\n",
    "    print(f'Removed {len_before - len_after} rows with NaN values')\n",
    "\n",
    "    X_train, Y_train, X_test, Y_test = generate_train_data(combined_data, SEQ_LEN, UP_FRONT_PREDICTION_DAYS, tech_indicator=TECH_INDICATOR, indicators=[TECH_INDICATOR], no_news=WITHOUT_NEWS, no_indicators=WITHOUT_INDICATORS)\n",
    "    model = build_model(SEQ_LEN, AMOUNT_OF_PARAMS, OPTIMIZER, LOSS_FUNCTION, ACTIVATION, AMOUNT_OF_NEURONS)\n",
    "    try:\n",
    "        model.fit(tf.convert_to_tensor(X_train,dtype=tf.float32), \n",
    "                tf.convert_to_tensor(Y_train,dtype=tf.float32), \n",
    "                epochs=EPOCHS,  \n",
    "                batch_size=128, \n",
    "                validation_batch_size = 64, \n",
    "                verbose='0')\n",
    "        loss = model.evaluate(tf.convert_to_tensor(X_test,dtype=tf.float32), \n",
    "                            tf.convert_to_tensor(Y_test,dtype=tf.float32), \n",
    "                            verbose='0', \n",
    "                            batch_size=128)\n",
    "        keras.backend.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "    except Exception as e:\n",
    "        output_in_logfile(f\"Error training {ticker}: {e}\")\n",
    "        continue    \n",
    "\n",
    "    # check if loss is float\n",
    "    loss_evaluation = \"good\"\n",
    "    theshold = 0.005\n",
    "    loss_value = 0.0\n",
    "    if isinstance(loss, float):\n",
    "        if loss > theshold:\n",
    "            loss_evaluation = \"bad\"\n",
    "            print(f'Loss for {ticker} is {loss}')\n",
    "            del model\n",
    "            continue\n",
    "        loss_value = round(float(loss), 10)\n",
    "        theshold = round(theshold, 10)\n",
    "    # Get the current date and time\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Format the date and time\n",
    "    formatted_date_time = now.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    indicator_str = TECH_INDICATOR\n",
    "    news_str = \"with_news\"\n",
    "    if WITHOUT_INDICATORS:\n",
    "        indicator_str = \"no_indicators\"\n",
    "    if WITHOUT_NEWS:    \n",
    "        news_str = \"no_news\"\n",
    "    else:\n",
    "        indicator_str = TECH_INDICATOR\n",
    "    os.makedirs(f'./models/LSTM/{ticker.lower()}', exist_ok=True)\n",
    "    model.save(f'./models/LSTM/{ticker.lower()}/model-{formatted_date_time}-{AMOUNT_OF_PARAMS}-{SEQ_LEN}-{AMOUNT_OF_NEURONS}-{OPTIMIZER}-{ACTIVATION}-{indicator_str}-{LOSS_FUNCTION}-{loss_evaluation}-{loss_value}-{theshold}-{news_str}-pending.keras')    \n",
    "\n",
    "\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d13164",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e89da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import sqlite3\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import Pool\n",
    "\n",
    "UP_FRONT_PREDICTION_DAYS = 5\n",
    "path_predictions_db = './data/predictions.db'\n",
    "\n",
    "def insert_prediction_into_db(ticker, date, predicted_stock_price, actual_stock_price, filename, with_news, indicator, optimizer, activation, loss_function, loss):\n",
    "    conn = sqlite3.connect(path_predictions_db, timeout=30)\n",
    "    execute_with_retry(conn, 'INSERT OR REPLACE INTO predictions VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', (ticker, date, predicted_stock_price, actual_stock_price, filename, with_news, indicator, optimizer, activation, loss_function, loss))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# def predict_stock_price(data, filename, ticker, date, actual_stock_price, tech_indicator, optimizer, activation, loss_function, no_news, no_indicators):\n",
    "def predict_stock_price(data_tuple):\n",
    "    data, filename, ticker, date, actual_stock_price, tech_indicator, optimizer, activation, loss_function, no_news, no_indicators = data_tuple\n",
    "    model = load_model(f'./bigdata/one/models/LSTM/{ticker.lower()}/{filename}')\n",
    "    try:    \n",
    "        X, scaler = generate_inference_data(data, tech_indicator=tech_indicator, no_news=no_news, no_indicators=no_indicators)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating inference data for {filename}: {e}\")\n",
    "        return 0\n",
    "    # print(f'X: {len(X)}')\n",
    "    # print(f'X shape: {X.shape}')\n",
    "    # print(f'Model input shape: {model.input_shape}')\n",
    "    try:\n",
    "        predicted_stock_price = model.predict(X, verbose=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting stock price for {filename}: {e}\")\n",
    "        return 0\n",
    "    dummy_array = np.zeros((1,X.shape[2]))\n",
    "    dummy_array[0][1] = predicted_stock_price[0][0]\n",
    "    predicted_stock_price = scaler.inverse_transform(dummy_array)[0][1]\n",
    "    with_news = 1 if no_news == False else 0\n",
    "    date_to_predict = date.strftime(\"%Y-%m-%d\")\n",
    "    insert_prediction_into_db(ticker, date_to_predict, predicted_stock_price, actual_stock_price, filename, with_news, tech_indicator, optimizer, activation, loss_function, loss)\n",
    "    del model\n",
    "    return predicted_stock_price\n",
    "\n",
    "\n",
    "\n",
    "path_stocks_db = 'data/stocks.db'\n",
    "conn_stocks = sqlite3.connect(path_stocks_db, timeout=30)\n",
    "\n",
    "thursdays = get_dates_of_day_of_week_in_range('2023-11-01', '2024-11-01', 3)\n",
    "traded_thursdays = filter_dates_not_in_stock_table(thursdays)\n",
    "\n",
    "\n",
    "path_bigdata_logs_db = 'bigdata/one/logs.db'\n",
    "conn = sqlite3.connect(path_bigdata_logs_db, timeout=30)\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT ticker, filename, amount_of_params, seq_len, amount_of_neurons_l1, amount_of_neurons_l2, amount_of_neurons_l3, optimizer, activation, indicator, loss_function, loss_evaluation, loss, threshold, with_news FROM log_models WHERE ticker = \"acgl\" AND loss < 0.005')\n",
    "res = c.fetchall()\n",
    "c.close()\n",
    "conn.close()\n",
    "print(f'Res: {len(res)}')\n",
    "\n",
    "\n",
    "\n",
    "for row in res:\n",
    "    ticker = row[0]\n",
    "    model_name = row[1]\n",
    "    amount_of_params = row[2]\n",
    "    seq_len = row[3]\n",
    "    amount_of_neurons_l1 = row[4]\n",
    "    amount_of_neurons_l2 = row[5]\n",
    "    amount_of_neurons_l3 = row[6]\n",
    "    optimizer = row[7]\n",
    "    activation = row[8]\n",
    "    indicator = row[9]\n",
    "    loss_function = row[10]\n",
    "    loss_evaluation = row[11]\n",
    "    loss = row[12]\n",
    "    threshold = row[13]\n",
    "    with_news = row[14]\n",
    "\n",
    "    no_news = with_news == 0\n",
    "    no_indicators = indicator == \"no_indicator\" or indicator == \"\"\n",
    "    \n",
    "    data_batch = []\n",
    "    for thursday in traded_thursdays:\n",
    "        date_to_predict = thursday\n",
    "        \n",
    "        conn = sqlite3.connect(path_predictions_db, timeout=30)\n",
    "        c = conn.cursor()\n",
    "        c.execute('SELECT 1 FROM predictions WHERE ticker = ? AND filename = ? AND date = ? ', (ticker, model_name, date_to_predict.strftime(\"%Y-%m-%d\")))\n",
    "        exists = c.fetchall()\n",
    "        c.close()\n",
    "        conn.close()\n",
    "        if len(exists) > 0:\n",
    "            print(f'Prediction for {ticker} {model_name} {date_to_predict} already exists')\n",
    "            continue\n",
    "\n",
    "        data, actual_stock_price = stock_data_to_df(\n",
    "            ticker, \n",
    "            date_to_predict, \n",
    "            seq_len = seq_len, \n",
    "            up_front_prediction_days=UP_FRONT_PREDICTION_DAYS, \n",
    "            tech_indicator=indicator, \n",
    "            no_news=no_news, \n",
    "            no_indicators=no_indicators)\n",
    "        data_batch.append((data, model_name, ticker, date_to_predict, actual_stock_price, indicator, optimizer, activation, loss_function, no_news, no_indicators))\n",
    "    # with Pool(processes=4) as pool:  # Adjust number of processes\n",
    "        # _ = pool.map(predict_stock_price, data_batch)\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        _ = list(executor.map(predict_stock_price, data_batch))    \n",
    "    # print(f'Results: {results}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbb858c",
   "metadata": {},
   "source": [
    "#### Create predictions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "688dc58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "path_predictions_db = './data/predictions.db'\n",
    "conn = sqlite3.connect(path_predictions_db, timeout=30)\n",
    "c = conn.cursor()\n",
    "try:\n",
    "    c.execute('DROP INDEX IF EXISTS idx_predictions_ticker_date_filename')\n",
    "    c.execute('DROP TABLE IF EXISTS predictions')\n",
    "    c.execute('VACUUM')\n",
    "    c.execute('CREATE TABLE EXISTS predictions (ticker text, date text, predicted_stock_price real, actual_stock_price real, filename text, with_news integer, indicator text, optimizer text, activation text, loss_function text, loss real) STRICT')\n",
    "    c.execute('CREATE INDEX EXISTS idx_predictions_ticker_date_filename ON predictions(ticker, filename, date)')\n",
    "except Exception as e:\n",
    "    print(f'Error creating predictions table: {e}')\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1070401",
   "metadata": {},
   "source": [
    "#### Precision of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e415959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, os\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Assuming y_true are the true labels and y_pred are the predicted labels\n",
    "\n",
    "conn = sqlite3.connect('data/predictions.db', timeout=30)\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT DISTINCT(ticker) FROM predictions')\n",
    "res = c.fetchall()\n",
    "ticker_list = [row[0] for row in res]\n",
    "c.close()\n",
    "precision_list  = []\n",
    "for ticker in ticker_list:\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT predicted_stock_price, actual_stock_price  FROM predictions where with_news = 1 and indicator != \"\" and predicted_stock_price is not null and actual_stock_price is not null and ticker = ?', (ticker,))\n",
    "    res = c.fetchall()\n",
    "    c.close()\n",
    "    counter += len(res)\n",
    "    # print(f'Counter: {counter}')\n",
    "    counter = 0\n",
    "    # average precision\n",
    "    y_true = []\n",
    "    y_pred = [] \n",
    "    ticker_list = []\n",
    "    precision_sum = 0\n",
    "    for row in res:\n",
    "        y_true.append(row[1])\n",
    "        y_pred.append(row[0])\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    error = (np.abs(y_true - y_pred)/y_true) * 100\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    precision_list.append((mape, ticker))\n",
    "    # print(f'Ticker: {ticker}; MAPE: {mape:.2f}%')\n",
    "conn.close()\n",
    "# remove nan from precision_list\n",
    "precision_list = [x for x in precision_list if not np.isnan(x[0])]\n",
    "precision_list.sort(key=lambda x: x[0])\n",
    "# median of precision_list\n",
    "median_value = np.median([x[0] for x in precision_list])\n",
    "for row in precision_list:\n",
    "    print(f'Ticker: {row[1]}; MAPE: {row[0]:.2f}%')\n",
    "print(f'Median MAPE: {median_value:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
